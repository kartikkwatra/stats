<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="lm_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="lm_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="lm_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="lm_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="lm_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="lm_files/navigation-1.1/tabsets.js"></script>
<link href="lm_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="lm_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">




</div>


<div id="the-model" class="section level1">
<h1>The model</h1>
<p>Simple linear regression is one of the most basic statistical models. There are several ways to describe the model. Here is one option: <span class="math display">\[y_i \sim \mathrm{Norm}(\mu_i,\sigma^2) \qquad \mathrm{for} \quad i=1,\dots,n \]</span> where <span class="math inline">\(\mu_i = \beta_0 + \beta_1 x_i\)</span> and <span class="math inline">\(x_i\)</span> is a continuous covariate.</p>
<p>Here’s another: <span class="math display">\[y_i = \beta_0 + \beta_1 x_i + \varepsilon_i\]</span> where <span class="math inline">\(\varepsilon_i \sim \mathrm{Norm}(0, \sigma^2)\)</span>.</p>
<p>A third option is to use matrix notation: <span class="math display">\[{\bf y} = {\bf X}{\boldsymbol \beta} + {\boldsymbol \varepsilon}\]</span> where <span class="math inline">\({\bf y}\)</span> is the response vector and <span class="math inline">\(\bf X\)</span> is the design matrix, a <span class="math inline">\(n \times p\)</span> matrix with the the first column being a vector of 1’s corresponding to the intercept and the other columns containing the covariates, which will be dummy variables for factors. In simple linear regression, there is only one covariate, so <span class="math inline">\(p=2\)</span>. The vector of coefficients to be estimated is denoted by <span class="math inline">\(\boldsymbol \beta\)</span>, and <span class="math inline">\(\boldsymbol \varepsilon\)</span> is the vector of residuals.</p>
</div>
<div id="inference" class="section level1">
<h1>Inference</h1>
<p>A linear regression can be fitted to the data using ordinary least squares (OLS), which is fast and convienient, but not generalizable to non-Gaussian problems. We will focus instead on maximum likelihood and MCMC for performing classical and Bayesian inference, respectively.</p>
<div id="classical-likelihood-based-approach" class="section level2">
<h2>Classical, likelihood-based approach</h2>
<p>The likelihood is the joint probability density of the data viewed as a function of the parameters. In this case, the probability density for a single observation is <span class="math inline">\(p(y_i|\beta_0,\beta_1,\sigma)=\mathrm{Norm}(\mu_i,\sigma^2)\)</span>, and under the standard independence assumption, the joint density is the product of the <span class="math inline">\(n\)</span> densities: <span class="math display">\[L(\beta_0,\beta_1,\sigma|{\bf y}) = \prod_{i=1}^n
p(y_i|\beta_0,\beta_1,\sigma)\]</span></p>
<p>In practice, the likelihood is evaluated on the log scale to avoid computational problems that can result from multiplying small probabilities. The log-likelihood is just this: <span class="math display">\[l(\beta_0,\beta_1,\sigma|y_i) = \sum_{i=1}^n
\log(p(y_i|\beta_0,\beta_1,\sigma))\]</span></p>
<p>Plugging in values for the parameters will return the negative log-likelihood for a particular dataset. Classical inference involves finding the parameters that maximize the likelihood. It’s easiest to let computers do the work, and <strong>R</strong> has many functions for the task.</p>
</div>
<div id="bayesian-approach" class="section level2">
<h2>Bayesian approach</h2>
<p>Bayesian inference is also based on the likelihood, but the goal is to characterize the posterior distribution of the parameters, given the data and a user specified prior distribution. The posterior distribution describes uncertainty about the parameters.</p>
<p>The posterior distribution of the linear model parameters is: <span class="math display">\[
p(\beta_0,\beta_1,\sigma|{\bf y}) \propto \left\lbrace \prod_{i=1}^n
p(y_i|\beta_0,\beta_1,\sigma)\right\rbrace p(\beta_0,\beta_1,\sigma)
\]</span> where the first term on the right-hand side of the equation should look familiar because it is the likelihood discussed above. The second term is the prior distribution of the parameters.</p>
<p>Normally, the prior distributions are taken to be independent of one another, and if little prior information is available, diffuse Gaussian or uniform distributions are often used to characterize the lack of knowledge. The prior distributions will have little effect on the posterior distribution if data are informative about the parameters of interest. However, it’s always important to assess the influence of the prior.</p>
<p>The computational challenge facing Bayesians is that it is rarely possible to compute the posterior distribution directly because it is a multivarite distribution with an intractible normalizing constant. This seemly enormous problem can be resolved using Markov chain Monte Carlo methods. Gibbs sampling is the most general MCMC technique, and it involves sequentially sampling each parameter from its full conditional distribution – the probability distribution of the parameter of interest, conditional on the data and all the other paramters in the model. For a linear model, a Gibbs sampler would involve repeating the following steps several thousand times:</p>
<hr />
<div id="step-1-sample-beta_0-from-its-full-conditional-distribution" class="section level4">
<h4>Step 1: Sample <span class="math inline">\(\beta_0\)</span> from its full conditional distribution:</h4>
<p><span class="math display">\[
p(\beta_0|\beta_1,\sigma) \propto \left\lbrace \prod_{i=1}^n
p(y_i|\beta_0,\beta_1,\sigma) \right\rbrace p(\beta_0)
\]</span></p>
</div>
<div id="step-2-sample-beta_1-from-its-full-conditional-distribution" class="section level4">
<h4>Step 2: Sample <span class="math inline">\(\beta_1\)</span> from its full conditional distribution:</h4>
<p><span class="math display">\[p
(\beta_1|\beta_0,\sigma) \propto \left\lbrace \prod_{i=1}^n
p(y_i|\beta_0,\beta_1,\sigma)\right\rbrace p(\beta_1)
\]</span></p>
</div>
<div id="step-3-sample-sigma-from-its-full-conditional-distribution" class="section level4">
<h4>Step 3: Sample <span class="math inline">\(\sigma\)</span> from its full conditional distribution:</h4>
<p><span class="math display">\[
p(\sigma|\beta_0,\beta_1) \propto \left\lbrace \prod_{i=1}^n
p(y_i|\beta_0,\beta_1,\sigma)\right\rbrace p(\sigma)
\]</span></p>
<hr />
<p>So, what do the full conditional distributions looks like? In this case, if conjugate prior distributions are used, the full conditional distributions are Gaussian for the <span class="math inline">\(\beta\)</span>’s and gamma for <span class="math inline">\(\sigma\)</span>. A great cheat sheet can be found <a href="https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions">here</a>. However, we often don’t want to restrict ourselves to conjugate priors, in which case we can use a Metropolis-Hastings algorithm to indirectly sample from each full conditional distribution. For example, we can propose <span class="math inline">\(\beta_{0}^{(c)} \sim \mathrm{Norm}(\beta_0, \mathrm{tune}_1)\)</span> and accept this candidate value with probability <span class="math inline">\(\min(1,R)\)</span> where <span class="math inline">\(R\)</span> is the MH acceptance ratio:</p>
<p><span class="math display">\[
R = \frac{\lbrace \prod_{i=1}^n p(y_i|\beta_0^{(c)},\beta_1,\sigma^2)\rbrace p(\beta_0^{(c)})p(\beta_0|\beta_0^{(c)})}{\lbrace \prod_{i=1}^n p(y_i|\beta_0,\beta_1,\sigma^2)\rbrace p(\beta_0)p(\beta_0^{(c)}|\beta_0)}
\]</span></p>
<p>Notice that the numerator and the denominator are made up of the product of the likelihood, the prior, and the proposal distributions. The likelihood and prior in the numerator are associated with the the candidate value. The proposal distribution in the numerator is the probability density associated with transitioning from <span class="math inline">\(\beta_0^{(c)}\)</span> back to <span class="math inline">\(\beta_0\)</span>. The denominator has the likelihood and prior of the current values, along with the probability density associated with moving to the candidate from the current value of <span class="math inline">\(\beta_0\)</span>. If a symmetric proposal distribution is used, the thrid terms in the numerator and denominator cancel out and do not need to be computed.</p>
</div>
</div>
</div>
<div id="example" class="section level1">
<h1>Example</h1>
<p>Here we demonstrate how to fit a simple linear model using both classical and Bayesian methods.</p>
<div id="simulate-a-dataset" class="section level2">
<h2>Simulate a dataset</h2>
<p>The following <strong>R</strong> code simulates the response variable <span class="math inline">\(y\)</span>, using some specified parameters and a randombly generated covariate <span class="math inline">\(x\)</span>. We will use this dataset to demonstrate maximum likelihood and MCMC methods.</p>
<pre class="r"><code>set.seed(348720) # To make this reproducible
n &lt;- 100
x &lt;- rnorm(n) # Covariate
beta0 &lt;- -1
beta1 &lt;- 1
sigma &lt;- 2

mu &lt;- beta0 + beta1*x     # expected value of y
y &lt;- rnorm(n, mu, sigma)  # realized values (ie, the response variable)</code></pre>
<p>Take a look:</p>
<pre class="r"><code>cbind(x,y)[1:4,] # First 4 observations</code></pre>
<pre><code>##                x          y
## [1,] -0.93295514 -0.2223842
## [2,] -0.02648071 -3.7537644
## [3,] -0.23166802 -0.7151488
## [4,]  1.64687862 -0.6357651</code></pre>
<pre class="r"><code>plot(x,y)</code></pre>
<p><img src="lm_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="classical-analysis" class="section level2">
<h2>Classical analysis</h2>
<p>Here is an R function to compute the negative log-likelihood.</p>
<pre class="r"><code>nll &lt;- function(pars) {
    beta0 &lt;- pars[1]
    beta1 &lt;- pars[2]
    sigma &lt;- pars[3]
    mu &lt;- beta0 + beta1*x
    ll &lt;- dnorm(y, mean=mu, sd=sigma, log=TRUE)
    -sum(ll)
}</code></pre>
<div id="minimize-the-negative-log-likelihood" class="section level3">
<h3>Minimize the negative log-likelihood</h3>
<p>Now that we have data and a likelihood function, we need to find the parameter values that maximize the log-likelihood, or equivalently, minimize the negative log-likelihood. Before we do that, note that we could try the brute force approach of guessing parameter values, evaluating the likelihood, and then repeating until we can’t lower the negative log-likelihood anymore. For example:</p>
<pre class="r"><code># Guess the parameter values and evalueate the likelihood
starts &lt;- c(beta0=0,beta1=0,sigma=1)
nll(starts)</code></pre>
<pre><code>## [1] 397.9182</code></pre>
<pre class="r"><code>## Another guess. This one is better because nll is lower
starts2 &lt;- c(beta0=-1,beta1=0,sigma=1)
nll(starts2)</code></pre>
<pre><code>## [1] 352.6342</code></pre>
<p>This is obviously a bad idea. Even with only three parameters, it would take forever to find the true maximum likelihood estimates (MLEs). Fortunately, there are many optimization functions in <strong>R</strong>. We’ll use <code>optim</code>, but <code>nlm</code> or <code>nlminb</code> would work just as well.</p>
<p>The <code>optim</code> function requires starting values and a likelihood function. If the likelihood function needs arguments other than the parameter vector, you can pass these to optim through the <code>...</code> argument. If you want standard errors, you need to compute the hessian matrix.</p>
<pre class="r"><code>fm &lt;- optim(starts, nll, hessian=TRUE)
fm</code></pre>
<pre><code>## $par
##      beta0      beta1      sigma 
## -0.8780081  0.8198882  2.1215936 
## 
## $value
## [1] 217.1089
## 
## $counts
## function gradient 
##       98       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL
## 
## $hessian
##               beta0         beta1        sigma
## beta0 22.2164990404 -2.0284602584 5.715464e-04
## beta1 -2.0284602584 23.7002731112 8.536745e-04
## sigma  0.0005715464  0.0008536745 4.443078e+01</code></pre>
<p>The <code>par</code> component has the MLEs. The <code>value</code> component is the negative log-likelihood at the MLEs. The <code>convergence</code> value should be 0. To obtain the SEs, we need to first invert the Hessian to get the variance-covariance matrix:</p>
<pre class="r"><code>vcov &lt;- solve(fm$hessian)
SEs &lt;- sqrt(diag(vcov))</code></pre>
<p>Now, let’s compare our results:</p>
<pre class="r"><code>mles &lt;- fm$par # The maximum likelihood estimates
cbind(Est=mles, SE=SEs)</code></pre>
<pre><code>##              Est        SE
## beta0 -0.8780081 0.2129932
## beta1  0.8198882 0.2062182
## sigma  2.1215936 0.1500231</code></pre>
<p>to results from <code>lm</code>:</p>
<pre class="r"><code>summary(fm1 &lt;- lm(y~x))</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.2770 -1.4190 -0.1121  1.3182  4.3638 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -0.8780     0.2152  -4.081 9.16e-05 ***
## x             0.8199     0.2083   3.936 0.000155 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.143 on 98 degrees of freedom
## Multiple R-squared:  0.1365, Adjusted R-squared:  0.1277 
## F-statistic: 15.49 on 1 and 98 DF,  p-value: 0.0001549</code></pre>
<p>The results are very similar. The small differences are likely due to the use of maximum likelihood instead of ordinary least-squares, which is used by <code>lm</code>.</p>
</div>
<div id="predictions" class="section level3">
<h3>Predictions</h3>
<p>Predictions of the expected value of <span class="math inline">\(y\)</span> for new values of <span class="math inline">\(x\)</span> can be found by plugging the MLEs back into the linear model. Here is an easy way to get predictions and confidence intervals using the <code>predict</code> function.</p>
<pre class="r"><code>xpred &lt;- seq(min(x), max(x), length.out=50)
ypredCI &lt;- predict(fm1, newdata=data.frame(x=xpred), interval=&quot;confidence&quot;)
plot(x,y)
lines(xpred, ypredCI[,&quot;fit&quot;])
lines(xpred, ypredCI[,&quot;lwr&quot;], lty=2)
lines(xpred, ypredCI[,&quot;upr&quot;], lty=2)</code></pre>
<p><img src="lm_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>If we repeated the study many times, the actual regression line (for the true values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> should lie within these confidence intervals 95 percent of the time.</p>
</div>
</div>
<div id="bayesian-analysis" class="section level2">
<h2>Bayesian analysis</h2>
<div id="a-gibbs-sampler-in-r" class="section level3">
<h3>A Gibbs sampler in <strong>R</strong></h3>
<pre class="r"><code>lm.gibbs &lt;- function(y, x, niter=10000, start, tune) {
samples &lt;- matrix(NA, niter, 3)
colnames(samples) &lt;- c(&quot;beta0&quot;, &quot;beta1&quot;, &quot;sigma&quot;)
beta0 &lt;- start[1]; beta1 &lt;- start[2]; sigma &lt;- start[3]

for(iter in 1:niter) {
    ## Sample from p(beta0|dot)
    mu &lt;- beta0 + beta1*x
    ll.y &lt;- sum(dnorm(y, mu, sigma, log=TRUE))
    prior.beta0 &lt;- dnorm(beta0, 0, 1000, log=TRUE)
    beta0.cand &lt;- rnorm(1, beta0, tune[1])
    mu.cand &lt;- beta0.cand + beta1*x
    ll.y.cand &lt;- sum(dnorm(y, mu.cand, sigma, log=TRUE))
    prior.beta0.cand &lt;- dnorm(beta0.cand, 0, 1000, log=TRUE)
    R &lt;- exp((ll.y.cand+prior.beta0.cand) - (ll.y+prior.beta0)) #MHR
    if(runif(1) &lt; R) {
        beta0 &lt;- beta0.cand
    }

    ## Sample from p(beta1|dot)
    mu &lt;- beta0 + beta1*x
    ll.y &lt;- sum(dnorm(y, mu, sigma, log=TRUE))
    prior.beta1 &lt;- dnorm(beta1, 0, 1000, log=TRUE)
    beta1.cand &lt;- rnorm(1, beta1, tune[2])
    mu.cand &lt;- beta0 + beta1.cand*x
    ll.y.cand &lt;- sum(dnorm(y, mu.cand, sigma, log=TRUE))
    prior.beta1.cand &lt;- dnorm(beta1.cand, 0, 1000, log=TRUE)
    R &lt;- exp((ll.y.cand+prior.beta1.cand) - (ll.y+prior.beta1))
    if(runif(1) &lt; R) {
        beta1 &lt;- beta1.cand
    }

    ## Sample from p(sigma|dot)
    ll.y &lt;- sum(dnorm(y, mu, sigma, log=TRUE))
    prior.sigma &lt;- dunif(sigma, 0, 1000, log=TRUE)
    sigma.cand &lt;- rlnorm(1, log(sigma), tune[3])
    mu &lt;- beta0 + beta1*x
    ll.y &lt;- sum(dnorm(y, mu, sigma, log=TRUE))
    prior.sigma &lt;- dunif(sigma, 0, 1000, log=TRUE)
    prop.sigma &lt;- dlnorm(sigma, log(sigma.cand), tune[3], log=TRUE)
    ll.y.cand &lt;- sum(dnorm(y, mu, sigma.cand, log=TRUE))
    prior.sigma.cand &lt;- dunif(sigma.cand, 0, 1000, log=TRUE)
    prop.sigma.cand &lt;- dlnorm(sigma.cand, log(sigma), tune[3], log=TRUE)
    R &lt;- exp((ll.y.cand+prior.sigma.cand+prop.sigma) -
               (ll.y+prior.sigma+prop.sigma.cand))
    if(runif(1) &lt; R) {
        sigma &lt;- sigma.cand
    }
    samples[iter,] &lt;- c(beta0, beta1, sigma)
}
return(samples)
}</code></pre>
<p>The function <code>lm.gibbs</code> is fairly long and dense. Take a look at the script <code>stats/keys/lm-key-old.R</code> to see an annotated function along with several other functions for making the algorithm much faster. These examples include the use of <code>Rcpp</code> and <code>RcppArmadillo</code>.</p>
<p>Here’s how to run the function:</p>
<pre class="r"><code>out1 &lt;- lm.gibbs(y=y, x=x, niter=1000,
                start=c(0,0,1),
                tune=c(0.4, 0.4, 0.2))</code></pre>
<p>The <code>coda</code> package makes it easy to look at the results:</p>
<pre class="r"><code>library(coda)
mc1 &lt;- mcmc(out1)
summary(mc1)</code></pre>
<pre><code>## 
## Iterations = 1:1000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 1000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##          Mean     SD Naive SE Time-series SE
## beta0 -0.8823 0.2306 0.007293        0.01449
## beta1  0.8195 0.2445 0.007732        0.01824
## sigma  2.1713 0.1678 0.005307        0.01243
## 
## 2. Quantiles for each variable:
## 
##         2.5%     25%     50%     75%   97.5%
## beta0 -1.322 -1.0248 -0.8839 -0.7279 -0.4266
## beta1  0.303  0.6608  0.8444  0.9826  1.2564
## sigma  1.831  2.0815  2.1724  2.2778  2.4836</code></pre>
<p>There are many things to take note of. The <code>Mean</code> is the posterior mean. The <code>SD</code> is the posterior standard deviation, which will be similar to the SE you would get from a classical analysis. The <code>Naive SE</code> and <code>Time-series SE</code> tell you about the Monte Carlo error associated with the posterior means. In Bayesian inference, point estimates aren’t the main object of inference. Instead, you want the entire posterior distribution, and the quantiles are helpful for summarizing the distributions. You can also view the posteriors (along with the trace plots) using the <code>plot</code> method in the <code>coda</code> package.</p>
<pre class="r"><code>plot(mc1)</code></pre>
<p><img src="lm_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>You can see that there is a short burn-in period that should be discarded. You can do that, and optionally thin the chain, using the <code>window</code> method:</p>
<pre class="r"><code>mc1b &lt;- window(mc1, start=101, thin=1)</code></pre>
<p>Other things you can do in the <code>coda</code> package include assessing convergence and looking at the rejection rate.</p>
<pre class="r"><code>rejectionRate(mc1b)</code></pre>
<pre><code>##     beta0     beta1     sigma 
## 0.4883204 0.4816463 0.6017798</code></pre>
<p>These should be closer to 0.65 to increase our effective sample size (Although, it doesn’t really matter in this case because the Monte Carlo error rate is already very low). Let’s rerun the sampler with new tuning values and this time using 2 chains run in parallel:</p>
<pre class="r"><code>library(parallel)
nCores &lt;- 2
cl &lt;- makeCluster(nCores)
clusterExport(cl, c(&quot;lm.gibbs&quot;, &quot;y&quot;, &quot;x&quot;))
clusterSetRNGStream(cl, 3479)
out &lt;- clusterEvalQ(cl, {
    mc &lt;- lm.gibbs(y=y, x=x, niter=1000,
                   start=c(0,0,1), tune=c(0.7,0.7,0.3))
    return(mc)
})
mcp &lt;- as.mcmc.list(lapply(out, function(x) mcmc(x)))</code></pre>
<pre class="r"><code>plot(mcp)</code></pre>
<p><img src="lm_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Looking at the chains is the best way to assess convergence, but you can look at diagnostics too:</p>
<pre class="r"><code>gelman.diag(mcp) # Point ests. should be &lt;1.1 or so</code></pre>
<pre><code>## Potential scale reduction factors:
## 
##       Point est. Upper C.I.
## beta0       1.02       1.02
## beta1       1.00       1.00
## sigma       1.01       1.03
## 
## Multivariate psrf
## 
## 1</code></pre>
<p>Close the connections</p>
<pre class="r"><code>stopCluster(cl)</code></pre>
</div>
</div>
<div id="using-jags" class="section level2">
<h2>Using <strong>JAGS</strong></h2>
<p>The first thing to do is create a text file with the model description. Mine is called <code>lm-JAGS.jag</code>, and it looks like this:</p>
<pre><code>model {

# Priors
beta0 ~ dnorm(0, 0.0001)
beta1 ~ dunif(-100, 100)
sigmaSq ~ dunif(0, 1000)
sigma &lt;- sqrt(sigmaSq)
tau &lt;- 1/sigmaSq  # dnorm wants precision instead of variance

for(i in 1:n) {
  mu[i] &lt;- beta0 + beta1*x[i]
  y[i] ~ dnorm(mu[i], tau)
}

}</code></pre>
<p>Now, we need to put the data in a named list.</p>
<pre class="r"><code>jd &lt;- list(y=y, x=x, n=n)
str(jd)</code></pre>
<pre><code>## List of 3
##  $ y: num [1:100] -0.222 -3.754 -0.715 -0.636 -0.522 ...
##  $ x: num [1:100] -0.933 -0.0265 -0.2317 1.6469 0.2916 ...
##  $ n: num 100</code></pre>
<p>Pick the parameters to monitor</p>
<pre class="r"><code>jp &lt;- c(&quot;beta0&quot;, &quot;beta1&quot;, &quot;sigma&quot;)</code></pre>
<p>Create a function to generate random initial values</p>
<pre class="r"><code>ji &lt;- function() {
    list(beta0=rnorm(1), beta1=rnorm(1), sigmaSq=runif(1))
}
ji()</code></pre>
<pre><code>## $beta0
## [1] -0.3763193
## 
## $beta1
## [1] 0.1359298
## 
## $sigmaSq
## [1] 0.07151817</code></pre>
<p>Compile the model with 3 chains and adapt.</p>
<pre class="r"><code>library(rjags)
jm &lt;- jags.model(&quot;lm-JAGS.jag&quot;, data=jd, inits=ji, n.chains=3,
                 n.adapt=1000)</code></pre>
<p>Draw 5000 posterior samples for each chain</p>
<pre class="r"><code>jc &lt;- coda.samples(jm, jp, n.iter=5000)</code></pre>
<p>Take a look</p>
<pre class="r"><code>summary(jc)</code></pre>
<pre><code>## 
## Iterations = 1001:6000
## Thinning interval = 1 
## Number of chains = 3 
## Sample size per chain = 5000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##          Mean     SD Naive SE Time-series SE
## beta0 -0.8791 0.2215 0.001809       0.001808
## beta1  0.8178 0.2129 0.001738       0.002247
## sigma  2.1817 0.1606 0.001312       0.001766
## 
## 2. Quantiles for each variable:
## 
##          2.5%     25%     50%     75%   97.5%
## beta0 -1.3156 -1.0290 -0.8794 -0.7278 -0.4533
## beta1  0.4027  0.6736  0.8180  0.9592  1.2369
## sigma  1.8966  2.0676  2.1720  2.2859  2.5167</code></pre>
<p>Continue sampling where we left off.</p>
<pre class="r"><code>jc2 &lt;- coda.samples(jm, jp, n.iter=1000)</code></pre>
<p>Visualize</p>
<pre class="r"><code>plot(jc2)</code></pre>
<p><img src="lm_files/figure-html/jc2-plot-1.png" width="672" /></p>
<div id="prediction" class="section level3">
<h3>Prediction</h3>
<p>Bayesian predictions are made using the posterior predictive distribution, which can be computed by applying the function of interest to each posterior sample.</p>
<pre class="r"><code>jc2mat &lt;- as.matrix(window(jc2, thin=10))   ## Put MCMC samples in a matrix
Ey &lt;- matrix(NA, length(xpred), nrow(jc2mat))
for(i in 1:nrow(jc2mat)) {
  Ey[,i] &lt;- jc2mat[i,&quot;beta0&quot;] + jc2mat[i,&quot;beta1&quot;]*xpred
}
plot(x,y)
matlines(xpred, Ey, col=rgb(0,1,0,0.1))
lines(xpred, apply(Ey, 1, quantile, prob=0.025), lty=2)
lines(xpred, apply(Ey, 1, quantile, prob=0.975), lty=2)</code></pre>
<p><img src="lm_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="assignment" class="section level1">
<h1>Assignment</h1>
<ol style="list-style-type: decimal">
<li>Simulate a dataset in <strong>R</strong> using two covariates with <span class="math inline">\(\beta_0=-1\)</span>, <span class="math inline">\(\beta_1=1\)</span>, <span class="math inline">\(\beta_2=0.5\)</span>, and <span class="math inline">\(\sigma^2=4\)</span>. Let <span class="math inline">\(n=100\)</span> be the sample size, and generate the two continuous covariate from standard normal distributions.</li>
<li>Write the equation for the likelihood in <span class="math inline">\(\LaTeX\)</span>.</li>
<li>Obtain the MLEs in <strong>R</strong> by minimizing the negative log-likelihood</li>
<li>Write the joint posterior distribution in <span class="math inline">\(\LaTeX\)</span></li>
<li>Describe a Gibbs sampler for obtaining posterior samples</li>
<li>Implement the Gibbs sampler in <strong>R</strong> using the dataset that you simulated earlier.</li>
<li>Use <strong>JAGS</strong> to fit the model.</li>
</ol>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
