\documentclass[11pt]{article}

\usepackage{parskip}
\usepackage[hmargin=1in,vmargin=1in]{geometry}
\usepackage[pdftex,hidelinks]{hyperref}


\begin{document}


\title{Exercise II: Fitting a generalized linear model using maximum likelihood and
  Gibbs sampling} 
\author{Richard Chandler \\ Warnell School of Forestry and Natural
  Resources \\ University of Georgia \\ \href{mailto:rchandler@warnell.uga.edu}{rchandler@warnell.uga.edu}
}
\date{\today}

\maketitle


\section*{Introduction}

These assignments are meant for new graduate students in my lab. They
assume that students have some basic familiarity with statistical
inference, but most students will need help getting through the first
few exercises before they are comfortable with the process of writing
likelihood equations and joint posterior distributions. 

There are likely to be mistakes in these exercises. Feel free to let
me know if you find any, and I will try to correct them.


\section*{The model}

Generalized linear models.

Here's a binomial GLM with a single covariate:
\[
  y_i \sim \mathrm{Binom}(N_i, \mu_i)
\]
where $\mu_i = \beta_0 + \beta_1 x_i$ and $x_i$ is a continuous
covariate. $N_i$ is known. In standard logistic regression, $N_i=1$,
and so $y_i$ is a binary random variable following a Bernoulli distribution.

\section*{Assignment}


\begin{enumerate}
  \item Write the equation for the likelihood in \LaTeX.
  \item Simulate a dataset in {\tt R} using $\beta_0=-1$, $\beta_1=1$,
    $\sigma^2=4$. Let $n=100$ be the sample size, and generate a
    single continuous covariate from a standard normal distribution.
  \item Obtain the MLEs in {\tt R} by minimizing the negative log-likelihood
  \item Write the joint posterior distribution in \LaTeX
  \item Describe a Gibbs sampler for obtaining posterior samples
  \item Implement the Gibbs sampler in {\tt R} using the dataset that
    you simulated earlier.
\end{enumerate}



\end{document}
