\documentclass[11pt]{article}

\usepackage{parskip}
\usepackage[hmargin=1in,vmargin=1in]{geometry}
\usepackage[pdftex,hidelinks]{hyperref}


\begin{document}


\title{Exercise I: Fitting a linear model using maximum likelihood and
  Gibbs sampling}
\author{Richard Chandler \\ Warnell School of Forestry and Natural
  Resources \\ University of Georgia \\ \href{mailto:rchandler@warnell.uga.edu}{rchandler@warnell.uga.edu}
}
\date{\today}

\maketitle


\section*{Introduction}

These assignments are meant for new graduate students in my lab. They
assume that students have some basic familiarity with statistical
inference, but most students will need help getting through the first
few exercises before they are comfortable with the process of writing
likelihood equations and joint posterior distributions.

There are likely to be mistakes in these exercises. Feel free to let
me know if you find any, and I will try to correct them.


\section*{The model}

Simple linear regression is one of the most basic statistical
models. There are several ways to describe the model. Here is one
option:
\[
  y_i \sim \mathrm{Norm}(\mu_i,\sigma^2)
\]
where $\mu_i = \beta_0 + \beta_1 x_i$ and $x_i$ is a continuous covariate.

Here's another:
\[
  y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]
where $\epsilon_i \sim \mathrm{Norm}(0, \sigma^2)$.

\section*{Assignment}


\begin{enumerate}
  \item Simulate a dataset in {\tt R} using $\beta_0=-1$, $\beta_1=1$,
    $\sigma^2=4$. Let $n=100$ be the sample size, and generate a
    single continuous covariate from a standard normal distribution.
  \item Write the equation for the likelihood in \LaTeX.
  \item Obtain the MLEs in {\tt R} by minimizing the negative log-likelihood
  \item Write the joint posterior distribution in \LaTeX
  \item Describe a Gibbs sampler for obtaining posterior samples
  \item Implement the Gibbs sampler in {\tt R} using the dataset that
    you simulated earlier.
  \item Use {\tt JAGS} to fit the model.
\end{enumerate}



\end{document}
