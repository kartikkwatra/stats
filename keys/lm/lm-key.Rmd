---
author:
- |
    Richard Chandler\
    Warnell School of Forestry and Natural Resources\
    University of Georgia\
    <rchandler@warnell.uga.edu>
title: |
    Key to Exercise I: Fitting a linear model using maximum likelihood and
    Gibbs sampling\
    The exercise can be found in: [stats/exercises/lm.txt]{}
---

Simulate a dataset
==================

Here's some [R]{} code to simulate $x$ and $y$:

```{R}
set.seed(348720) # To make this reproducible
n <- 100
x <- rnorm(n) # Covariate
beta0 <- -1
beta1 <- 1
sigma <- 2

mu <- beta0 + beta1*x     # expected value of y
y <- rnorm(n, mu, sigma)  # realized values (ie, the response variable)
```


Take a look:

```{R}
cbind(x,y)[1:4,] # First 4 observations
plot(x,y)
```


Likelihood
==========

The likelihood is the product of the $n$ Gaussian densities:
$L(\beta_0,\beta_1,\sigma^2; {\bf y}) = \prod_{i=1}^n p(y_i|\beta_0,\beta_1,\sigma^2)$
where $p(y_i|\beta_0,\beta_1,\sigma^2) =
\text{Norm}(y_i|\mu_i,\sigma^2)$ and $\mu_i = \beta_0 +
\beta_1x_i$. The log-likelihood looks like this:
$$\ell(\beta_0,\beta_1,\sigma^2; {\bf y}) = \sum_{i=1}^n \log(p(y_i|\beta_0,\beta_1,\sigma^2))$$

Here is an R function to compute the negative log-likelihood:

```{R}
nll <- function(pars) {
    beta0 <- pars[1]
    beta1 <- pars[2]
    sigma <- pars[3]
    mu <- beta0 + beta1*x
    ll <- dnorm(y, mean=mu, sd=sigma, log=TRUE)
    -sum(ll)
}
```

Minimize the negative log-likelihood
====================================

Now that we have data and a likelihood function, we need to find the
parameter values that maximize the log-likelihood, or equivalently,
minimize the negative log-likelihood. Before we do that, note that we
could try the brute force approach of guessing parameter values,
evaluating the likelihood, and then repeating until we can't lower the
negative log-likelihood anymore. For example:

```{R}
# Guess the parameter values and evalueate the likelihood
starts <- c(beta0=0,beta1=0,sigma=1)
nll(starts)

## Another guess. This one is better because nll is lower
starts2 <- c(beta0=-1,beta1=0,sigma=1)
nll(starts2)
```

This is obviously a bad idea. Even with only three parameters, it would
take forever to find the true maximum likelihood estimates (MLEs).
Fortunately, there are many optimization functions in `R`. We'll use
`optim`, but `nlm` or `nlminb` would work just as well.

The `optim` function requires starting values and a likelihood function.
If the likelihood function needs arguments other than the parameter
vector, you can pass these to optim through the `...` argument. If you
want standard errors, you need to compute the hessian matrix.

```{R}
fm <- optim(starts, nll, hessian=TRUE)
fm
```

The `par` component has the MLEs. The `value` component is the negative
log-likelihood at the MLEs. The `convergence` value should be 0. To
obtain the SEs, we need to first invert the Hessian to get the
variance-covariance matrix:

```{R}
vcov <- solve(fm$hessian)
SEs <- sqrt(diag(vcov))
```

Now, let's compare our results:

```{R}
mles <- fm$par # The maximum likelihood estimates
cbind(Est=mles, SE=SEs)
```

to results from `lm`:

```{R}
summary(lm(y~x))
```

The results are very similar. The small differences are likely due to
the use of maximum likelihood instead of ordinary least-squares, which
is used by `lm`.

Joint posterior distribution and Gibbs sampling
===============================================

The joint posterior distribution is proportional to the product of the
likelihood and the joint prior distribution. The priors are usually
taken to be independent, so in this case we have:
$p(\beta_0,\beta_1,\sigma^2)=p(\beta_0)p(\beta1)p(\sigma^2)$, which
means that we can write the posterior like this:
$$p(\beta_0,\beta_1,\sigma^2 | {\bf y}) \propto \left\{\prod_{i=1}^n p(y_i|\beta_0,\beta_1,\sigma^2)\right\}p(\beta_0)p(\beta_1)p(\sigma^2)$$
where, as before,
$p(y_i|\beta_0,\beta_1,\sigma^2) = \text{Norm}(y_i|\mu_i,\sigma^2)$.
Here are three possibilities for the priors: $p(\beta_0) =
\text{Norm}(0,1000000)$, $p(\beta_1) = \rm{Norm}(0,1000000)$,
$p(\sigma) = \text{Unif}(0,1000)$. It's easy to show that the
influence of the prior is negligible for moderate to large sample sizes.

We can't easily compute the joint posterior distribution analytically
because it would require computing the normalizing constant in the
previous equation. To do that, we would have to do a three-dimensional
integration over the parameters. Fortunately, we can use MCMC to
overcome the problem posed by intractable normalizing constants. Gibbs
sampling is a type of MCMC algorithm that requires sampling each
parameter from its full conditional distribution. The full conditional
distribution for $\beta_0$ is:
$$p(\beta_0|\beta_1,\sigma^2,{\bf y}) \propto \left\{\prod_{i=1}^n p(y_i|\beta_0,\beta_1,\sigma^2)\right\}p(\beta_0)$$
This is the probability distribution for $\beta_0$, conditional on all
the other parameters in the model and the data. We can sample from this
distribution using the Metropolis-Hastings (MH) algorighm. For example,
we can propose $\beta_0^{*} \sim \text{Norm}(\beta_0,
\text{tune}_1)$ and accept this candidate value with probability
$\min(1,R)$ where $R$ is the MH acceptance ratio:
$$R = \frac{\left\{\prod_{i=1}^n p(y_i|\beta_0^{*},\beta_1,\sigma^2)\right\}p(\beta_0^{*})p(\beta_0|\beta_0^{*})}{\left\{\prod_{i=1}^n p(y_i|\beta_0,\beta_1,\sigma^2)\right\}p(\beta_0)p(\beta_0^{*}|\beta_0)}$$
Notice that the numerator and the denominator are made up of the product
of the likelihood, the prior, and the proposal distributions. The
likelihood and prior in the numerator are associated with the the
candidate value. The proposal distribution in the numerator is the
probability density associated with transitioning from $\beta_0^{*}$
back to $\beta_0$. The denominator has the likelihood and prior of the
current values, along with the probability density associated with
moving to the candidate from the current value of $\beta_0$.

Sampling from the full conditional distributions of the other two
parameters can be achieved in a similar fashion. Here are the other two
full conditionals:
$$p(\beta_1|\beta_0,\sigma^2,{\bf y}) \propto \left\{\prod_{i=1}^n p(y_i|\beta_0,\beta_1,\sigma^2)\right\}p(\beta_1)$$

$$p(\sigma^2|\beta_0,\sigma^2,{\bf y}) \propto \left\{\prod_{i=1}^n p(y_i|\beta_0,\beta_1,\sigma^2)\right\}p(\sigma^2)$$

A few things to note about the Metropolis-Hastings algorithm. First, if
the proposal distribution is symmetric, you can ignore it when computing
$R$. Second, if you use conjugate priors, you can often sample directly
from the full conditional distributions rather than use the MH
algorithm. Here's a link to a handy cheat-sheat for conjugate priors:
<https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions>.
The last thing to mention about the HM algorithm is that you want to
accept about 30-40% of the proposals, and you therefore have to â€˜tune'
the algorithm to make it efficient. This means fiddling with the
parameter $\text{tune}_1$ shown above. It's usually pretty easy to
find good tuning values, but you can also use an adaptive phase to do
this automatically.

A Gibbs sampler in [R]{}
========================

```{R}
lm.gibbs <- function(y, x, niter=10000, start, tune) {
samples <- matrix(NA, niter, 3)
colnames(samples) <- c("beta0", "beta1", "sigma")
beta0 <- start[1]; beta1 <- start[2]; sigma <- start[3]

for(iter in 1:niter) {
    ## Sample from p(beta0|dot)
    mu <- beta0 + beta1*x
    ll.y <- sum(dnorm(y, mu, sigma, log=TRUE))
    prior.beta0 <- dnorm(beta0, 0, 1000, log=TRUE)
    beta0.cand <- rnorm(1, beta0, tune[1])
    mu.cand <- beta0.cand + beta1*x
    ll.y.cand <- sum(dnorm(y, mu.cand, sigma, log=TRUE))
    prior.beta0.cand <- dnorm(beta0.cand, 0, 1000, log=TRUE)
    mhr <- exp((ll.y.cand+prior.beta0.cand) - (ll.y+prior.beta0))
    if(runif(1) < mhr) {
        beta0 <- beta0.cand
    }

    ## Sample from p(beta1|dot)
    mu <- beta0 + beta1*x
    ll.y <- sum(dnorm(y, mu, sigma, log=TRUE))
    prior.beta1 <- dnorm(beta1, 0, 1000, log=TRUE)
    beta1.cand <- rnorm(1, beta1, tune[2])
    mu.cand <- beta0 + beta1.cand*x
    ll.y.cand <- sum(dnorm(y, mu.cand, sigma, log=TRUE))
    prior.beta1.cand <- dnorm(beta1.cand, 0, 1000, log=TRUE)
    mhr <- exp((ll.y.cand+prior.beta1.cand) - (ll.y+prior.beta1))
    if(runif(1) < mhr) {
        beta1 <- beta1.cand
    }

    ## Sample from p(sigma|dot)
    ll.y <- sum(dnorm(y, mu, sigma, log=TRUE))
    prior.sigma <- dunif(sigma, 0, 1000, log=TRUE)
    sigma.cand <- rlnorm(1, log(sigma), tune[3])
    mu <- beta0 + beta1*x
    ll.y <- sum(dnorm(y, mu, sigma, log=TRUE))
    prior.sigma <- dunif(sigma, 0, 1000, log=TRUE)
    prop.sigma <- dlnorm(sigma, log(sigma.cand), tune[3], log=TRUE)
    ll.y.cand <- sum(dnorm(y, mu, sigma.cand, log=TRUE))
    prior.sigma.cand <- dunif(sigma.cand, 0, 1000, log=TRUE)
    prop.sigma.cand <- dlnorm(sigma.cand, log(sigma), tune[3], log=TRUE)
    mhr <- exp((ll.y.cand+prior.sigma.cand+prop.sigma) -
               (ll.y+prior.sigma+prop.sigma.cand))
    if(runif(1) < mhr) {
        sigma <- sigma.cand
    }
    samples[iter,] <- c(beta0, beta1, sigma)
}
return(samples)
}
```

The function `lm.gibbs` is fairly long and dense. Take a look at the
script `stats/keys/lm-key-old.R` to see an annotated function along with
several other functions for making the algorithm much faster. These
examples include the use of [Rcpp]{} and [ RcppArmadillo]{}.

Here's how to run the function:

```{R}
out1 <- lm.gibbs(y=y, x=x, niter=1000,
                start=c(0,0,1),
                tune=c(0.4, 0.4, 0.2))
```

The `coda` package makes it easy to look at the results:

```{R}
library(coda)
mc1 <- mcmc(out1)
summary(mc1)
```

There are many things to take note of. The `Mean` is the posterior mean.
The `SD` is the posterior standard deviation, which will be similar to
the SE you would get from a classical analysis. The `Naive SE` and
`Time-series SE` tell you about the Monte Carlo error associated with
the posterior means. In Bayesian inference, point estimates aren't the
main object of inference. Instead, you want the entire posterior
distribution, and the quantiles are helpful for summarizing the
distributions. You can also view the posteriors (along with the trace
plots) using the `plot` method in the `coda` package.

```{R}
plot(mc1)
```


You can see that there is a short burn-in period that should be
discarded. You can do that, and optionally thin the chain, using the
`window` method:

```{R}
mc1b <- window(mc1, start=101, thin=1)
```

Other things you can do in the `coda` package include assessing
convergence and looking at the rejection rate.

```{R}
rejectionRate(mc1b)
```



These should be closer to 0.65 to increase our effective sample
size[^1]. Let's rerun the sampler with new tuning values and this time
using 2 chains run in parallel:

```{R}
library(parallel)
nCores <- 2
cl <- makeCluster(nCores)
clusterExport(cl, c("lm.gibbs", "y", "x"))
clusterSetRNGStream(cl, 3479)
out <- clusterEvalQ(cl, {
    mc <- lm.gibbs(y=y, x=x, niter=1000,
                   start=c(0,0,1), tune=c(0.7,0.7,0.3))
    return(mc)
})
mcp <- as.mcmc.list(lapply(out, function(x) mcmc(x)))
```

Looking at the chains is the best way to assess convergence, but you can
look at diagnostics too:

```{R}
plot(mcp)
```


Close the connections

```{R}
stopCluster(cl)
```


Using [JAGS]{}
==============

The first thing to do is create a text file with the model description.
Mine is called `lm-JAGS.jag`, and it looks like this:

Now we need to put the data in a named list, choose the parameters we
want to monitor, and create a function to generate random initial
values.

```{R}
jd <- list(y=y, x=x, n=n)
str(jd)

jp <- c("beta0", "beta1", "sigma")

ji <- function() {
    list(beta0=rnorm(1), beta1=rnorm(1), sigmaSq=runif(1))
}
ji()
```

Here's how to compile the model with 3 chains, adapt, and then draw
5000 posterior samples for each chain.

```{R results='hide'}
library(rjags)
jm <- jags.model("lm-JAGS.jag", data=jd, inits=ji, n.chains=3,
                 n.adapt=1000)
jc <- coda.samples(jm, jp, n.iter=5000)
```

Take a look:
```{R}
summary(jc)
````

Continue sampling where we left off.
```{R results='hide'}
jc2 <- coda.samples(jm, jp, n.iter=1000)

````


Visualize

```{R}
plot(jc2)
```

Discussion
===========

The keys to other exercises either won't be made publically available,
or they won't include so much explanation.

[^1]: Although, it doesn't really matter in this case because the Monte
    Carlo error rate is already very low
