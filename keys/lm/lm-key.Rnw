%% NOTE: If you have LaTeX installed, you can create a PDF from this
%% file by opening R, installing the 'knitr' package and running the
%% commands:
%% knitr::knit("lm-key.Rnw")
%% tools::texi2pdf("lm-key.tex")


\documentclass[12pt]{article}

\usepackage{parskip}
\usepackage[hmargin=1in,vmargin=1in]{geometry}
\usepackage[pdftex,hidelinks]{hyperref}
\usepackage{verbatim}


\begin{document}


\title{Key to Exercise I: Fitting a linear model using maximum
  likelihood and Gibbs sampling \\ \vspace{1cm}
  The exercise can be found in: {\tt stats/exercises/lm.txt}}
\author{Richard Chandler \\ Warnell School of Forestry and Natural
  Resources \\ University of Georgia \\ \href{mailto:rchandler@warnell.uga.edu}{rchandler@warnell.uga.edu}
}
\date{\today}

\maketitle





\section{Simulate a dataset}


Here's some {\tt R} code to simulate $x$ and $y$:

<<>>=
set.seed(348720) # To make this reproducible
n <- 100
x <- rnorm(n) # Covariate
beta0 <- -1
beta1 <- 1
sigma <- 2

mu <- beta0 + beta1*x     # expected value of y
y <- rnorm(n, mu, sigma)  # realized values (ie, the response variable)
@

\newpage

%From now on, let's pretend like this is data that we collected.
%<<>>=
%mydata <- data.frame(y, x)
%mydata[1:4,]
%@

%\newpage

Take a look:
<<scat>>=
cbind(x,y)[1:4,] # First 4 observations
plot(x,y)
@

%Imagine $x$ is (standardized) length and $y$ is mass, which we've
%recorded on 100 Canada warblers ({\it Cardellina canadensis}).


\newpage


\section{Likelihood}

The likelihood is the product of the $n$ Gaussian densities:
\[
  L(\beta_0,\beta_1,\sigma^2; {\bf y}) = \prod_{i=1}^n p(y_i|\beta_0,\beta_1,\sigma^2)
\]
where $p(y_i|\beta_0,\beta_1,\sigma^2) =
\mathrm{Norm}(y_i|\mu_i,\sigma^2)$ and $\mu_i = \beta_0 +
\beta_1x_i$. The log-likelihood looks like this:
\[
  \l(\beta_0,\beta_1,\sigma^2; {\bf y}) = \sum_{i=1}^n \log(p(y_i|\beta_0,\beta_1,\sigma^2))
\]



Here is an R function to compute the negative log-likelihood:
<<>>=
nll <- function(pars) {
    beta0 <- pars[1]
    beta1 <- pars[2]
    sigma <- pars[3]
    mu <- beta0 + beta1*x
    ll <- dnorm(y, mean=mu, sd=sigma, log=TRUE)
    -sum(ll)
}
@




\section{Minimize the negative log-likelihood}

Now that we have data and a likelihood function, we need to find the
parameter values that maximize the log-likelihood, or equivalently,
minimize the negative log-likelihood. Before we do that, note that we
could try the brute force approach of guessing parameter values,
evaluating the likelihood, and then repeating until we can't lower the
negative log-likelihood anymore. For example:

<<>>=
# Guess the parameter values and evalueate the likelihood
starts <- c(beta0=0,beta1=0,sigma=1)
nll(starts)

## Another guess. This one is better because nll is lower
starts2 <- c(beta0=-1,beta1=0,sigma=1)
nll(starts2)
@

This is obviously a bad idea. Even with only three parameters, it
would take forever to find the true maximum likelihood estimates
(MLEs). Fortunately, there are many optimization functions in
\verb+R+. We'll use \verb+optim+, but \verb+nlm+ or \verb+nlminb+
would work just as well.

The \verb+optim+ function requires starting values and a likelihood
function. If the likelihood function needs arguments other than the
parameter vector, you can pass these to optim through the \verb+...+
argument. If you want standard errors, you need to compute the hessian
matrix.
<<>>=
fm <- optim(starts, nll, hessian=TRUE)
fm
@
The \verb+par+ component has the MLEs. The \verb+value+ component is
the negative log-likelihood at the MLEs. The \verb+convergence+ value
should be 0. To obtain the SEs, we need to first invert the Hessian to
get the variance-covariance matrix:

<<>>=
vcov <- solve(fm$hessian)
SEs <- sqrt(diag(vcov))
@

Now, let's compare our results:

<<>>=
mles <- fm$par # The maximum likelihood estimates
cbind(Est=mles, SE=SEs)
@

to results from \verb+lm+:
<<>>=
summary(lm(y~x))
@

The results are very similar. The small differences are likely due to
the use of maximum likelihood instead of ordinary least-squares, which
is used by \verb+lm+.


\section{Joint posterior distribution and Gibbs sampling}

The joint posterior distribution is proportional to the product of the
likelihood and the joint prior distribution. The priors are usually
taken to be independent, so in this case we have:
$p(\beta_0,\beta_1,\sigma^2)=p(\beta_0)p(\beta1)p(\sigma^2)$, which
means that we can write the posterior like this:
\[
  p(\beta_0,\beta_1,\sigma^2 | {\bf y}) \propto \left\{\prod_{i=1}^n p(y_i|\beta_0,\beta_1,\sigma^2)\right\}p(\beta_0)p(\beta_1)p(\sigma^2)
\]
where, as before, $p(y_i|\beta_0,\beta_1,\sigma^2) = \mathrm{Norm}(y_i|\mu_i,\sigma^2)$.
Here are three possibilities for the priors: $p(\beta_0) =
\mathrm{Norm}(0,1000000)$, $p(\beta_1) = \mathrm{Norm}(0,1000000)$,
$p(\sigma) = \mathrm{Unif}(0,1000)$. It's easy to show that the
influence of the prior is negligible for moderate to large sample sizes.

We can't easily compute the joint posterior distribution
analytically because it would require computing the normalizing
constant in the previous equation. To do that, we would have to do a
three-dimensional integration over the parameters. Fortunately,
we can use MCMC to overcome the problem posed by intractable
normalizing constants. Gibbs sampling is a type of MCMC algorithm that
requires sampling each parameter from its full conditional
distribution. The full conditional distribution for $\beta_0$ is:
\[
  p(\beta_0|\beta_1,\sigma^2,{\bf y}) \propto \left\{\prod_{i=1}^n p(y_i|\beta_0,\beta_1,\sigma^2)\right\}p(\beta_0)
\]
This is the probability distribution for $\beta_0$, conditional on all
the other parameters in the model and the data. We can sample from
this distribution using the Metropolis-Hastings (MH) algorighm. For
example, we can propose $\beta_0^{*} \sim \mathrm{Norm}(\beta_0,
\mathrm{tune}_1)$ and accept this candidate value with probability
$\min(1,R)$ where $R$ is the MH acceptance ratio:
\[
  R = \frac{\left\{\prod_{i=1}^n p(y_i|\beta_0^{*},\beta_1,\sigma^2)\right\}p(\beta_0^{*})p(\beta_0|\beta_0^{*})}{\left\{\prod_{i=1}^n p(y_i|\beta_0,\beta_1,\sigma^2)\right\}p(\beta_0)p(\beta_0^{*}|\beta_0)}
\]
Notice that the numerator and the denominator are made up of the
product of the likelihood, the prior, and the proposal
distributions. The likelihood and prior in the numerator are
associated with the the candidate value. The proposal distribution in
the numerator is the probability density associated with transitioning
from $\beta_0^{*}$ back to $\beta_0$. The denominator has the
likelihood and prior of the current values, along with the probability
density associated with moving to the candidate from the current value
of $\beta_0$.

Sampling from the full conditional distributions of the other two
parameters can be achieved in a similar fashion. Here are the other
two full conditionals:
\[
  p(\beta_1|\beta_0,\sigma^2,{\bf y}) \propto \left\{\prod_{i=1}^n p(y_i|\beta_0,\beta_1,\sigma^2)\right\}p(\beta_1)
\]

\[
  p(\sigma^2|\beta_0,\sigma^2,{\bf y}) \propto \left\{\prod_{i=1}^n p(y_i|\beta_0,\beta_1,\sigma^2)\right\}p(\sigma^2)
\]

A few things to note about the Metropolis-Hastings algorithm. First,
if the proposal distribution is symmetric, you can ignore it when
computing $R$. Second, if you use conjugate priors, you can often
sample directly from the full conditional distributions rather than
use the MH algorithm. Here's a link to a handy cheat-sheat for
conjugate priors:
\url{https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions}.
The last thing to mention about the HM algorithm
is that you want to accept about 30-40\% of the proposals, and you
therefore have to `tune' the algorithm to make it efficient. This
means fiddling with the parameter $\mathrm{tune}_1$ shown above. It's
usually pretty easy to find good tuning values, but you can also use
an adaptive phase to do this automatically.


\newpage
\section{A Gibbs sampler in {\tt R}}

%\footnotesize
\scriptsize
<<>>=
lm.gibbs <- function(y, x, niter=10000, start, tune) {
samples <- matrix(NA, niter, 3)
colnames(samples) <- c("beta0", "beta1", "sigma")
beta0 <- start[1]; beta1 <- start[2]; sigma <- start[3]

for(iter in 1:niter) {
    ## Sample from p(beta0|dot)
    mu <- beta0 + beta1*x
    ll.y <- sum(dnorm(y, mu, sigma, log=TRUE))
    prior.beta0 <- dnorm(beta0, 0, 1000, log=TRUE)
    beta0.cand <- rnorm(1, beta0, tune[1])
    mu.cand <- beta0.cand + beta1*x
    ll.y.cand <- sum(dnorm(y, mu.cand, sigma, log=TRUE))
    prior.beta0.cand <- dnorm(beta0.cand, 0, 1000, log=TRUE)
    mhr <- exp((ll.y.cand+prior.beta0.cand) - (ll.y+prior.beta0))
    if(runif(1) < mhr) {
        beta0 <- beta0.cand
    }

    ## Sample from p(beta1|dot)
    mu <- beta0 + beta1*x
    ll.y <- sum(dnorm(y, mu, sigma, log=TRUE))
    prior.beta1 <- dnorm(beta1, 0, 1000, log=TRUE)
    beta1.cand <- rnorm(1, beta1, tune[2])
    mu.cand <- beta0 + beta1.cand*x
    ll.y.cand <- sum(dnorm(y, mu.cand, sigma, log=TRUE))
    prior.beta1.cand <- dnorm(beta1.cand, 0, 1000, log=TRUE)
    mhr <- exp((ll.y.cand+prior.beta1.cand) - (ll.y+prior.beta1))
    if(runif(1) < mhr) {
        beta1 <- beta1.cand
    }

    ## Sample from p(sigma|dot)
    ll.y <- sum(dnorm(y, mu, sigma, log=TRUE))
    prior.sigma <- dunif(sigma, 0, 1000, log=TRUE)
    sigma.cand <- rlnorm(1, log(sigma), tune[3])
    mu <- beta0 + beta1*x
    ll.y <- sum(dnorm(y, mu, sigma, log=TRUE))
    prior.sigma <- dunif(sigma, 0, 1000, log=TRUE)
    prop.sigma <- dlnorm(sigma, log(sigma.cand), tune[3], log=TRUE)
    ll.y.cand <- sum(dnorm(y, mu, sigma.cand, log=TRUE))
    prior.sigma.cand <- dunif(sigma.cand, 0, 1000, log=TRUE)
    prop.sigma.cand <- dlnorm(sigma.cand, log(sigma), tune[3], log=TRUE)
    mhr <- exp((ll.y.cand+prior.sigma.cand+prop.sigma) -
               (ll.y+prior.sigma+prop.sigma.cand))
    if(runif(1) < mhr) {
        sigma <- sigma.cand
    }
    samples[iter,] <- c(beta0, beta1, sigma)
}
return(samples)
}
@

\normalsize


The function \verb+lm.gibbs+ is fairly long and dense. Take a look at
the script \verb+stats/keys/lm-key-old.R+ to see an annotated function
along with several other functions for making the algorithm much
faster. These examples include the use of {\tt Rcpp} and {\tt
  RcppArmadillo}.

\newpage

Here's how to run the function:

<<>>=
out1 <- lm.gibbs(y=y, x=x, niter=1000,
                start=c(0,0,1),
                tune=c(0.4, 0.4, 0.2))
@


The \verb+coda+ package makes it easy to look at the results:
<<>>=
library(coda)
mc1 <- mcmc(out1)
summary(mc1)
@

There are many things to take note of. The \verb+Mean+ is the
posterior mean. The \verb+SD+ is the posterior standard deviation,
which will be similar to the SE you would get from a classical
analysis. The \verb+Naive SE+ and \verb+Time-series SE+ tell you about
the Monte Carlo error associated with the posterior means. In Bayesian
inference, point estimates aren't the main object of
inference. Instead, you want the entire posterior distribution, and
the quantiles are helpful for summarizing the distributions. You can
also view the posteriors (along with the trace plots) using the
\verb+plot+ method in the \verb+coda+ package.



\newpage


<<post>>=
plot(mc1)
@

You can see that there is a short burn-in period that should be
discarded. You can do that, and optionally thin the chain, using the
\verb+window+ method:
<<>>=
mc1b <- window(mc1, start=101, thin=1)
@

Other things you can do in the \verb+coda+ package include assessing
convergence and looking at the rejection rate.
<<>>=
rejectionRate(mc1b)
@
These should be closer to 0.65 to increase our effective sample
size\footnote{Although, it doesn't really matter in this case because
  the Monte Carlo error rate is already very low}. Let's rerun the
sampler with new tuning values and this time using 2 chains run in parallel:
<<>>=
library(parallel)
nCores <- 2
cl <- makeCluster(nCores)
clusterExport(cl, c("lm.gibbs", "y", "x"))
clusterSetRNGStream(cl, 3479)
out <- clusterEvalQ(cl, {
    mc <- lm.gibbs(y=y, x=x, niter=1000,
                   start=c(0,0,1), tune=c(0.7,0.7,0.3))
    return(mc)
})
mcp <- as.mcmc.list(lapply(out, function(x) mcmc(x)))
@


<<postp>>=
plot(mcp)
@

Looking at the chains is the best way to assess convergence, but you
can look at diagnostics too:
<<>>=
gelman.diag(mcp) # Point ests. should be <1.1 or so
@

Close the connections
<<>>=
stopCluster(cl)
@



\section{Using {\tt JAGS}}


The first thing to do is create a text file with the model
description. Mine is called \verb+lm-JAGS.jag+, and it looks like
this:

\fbox{\parbox{\linewidth}{\verbatiminput{lm-JAGS.jag}}}


Now, we need to put the data in a named list.
<<>>=
jd <- list(y=y, x=x, n=n)
str(jd)
@

Pick the parameters to monitor
<<>>=
jp <- c("beta0", "beta1", "sigma")
@

Create a function to generate random initial values
<<>>=
ji <- function() {
    list(beta0=rnorm(1), beta1=rnorm(1), sigmaSq=runif(1))
}
ji()
@


Compile the model with 3 chains and adapt.
<<jm,results='hide'>>=
library(rjags)
jm <- jags.model("lm-JAGS.jag", data=jd, inits=ji, n.chains=3,
                 n.adapt=1000)
@

Draw 5000 posterior samples for each chain
<<jc,results='hide'>>=
jc <- coda.samples(jm, jp, n.iter=5000)
@

Take a look
<<>>=
summary(jc)
@


Continue sampling where we left off.
<<jc2,results='hide'>>=
jc2 <- coda.samples(jm, jp, n.iter=1000)
@

Visualize
<<jc2-plot>>=
plot(jc2)
@



\section{Conclusions}


All of these methods give very similar results. Be wary of ideological
people that try to run down either classical or Bayesian approaches to
statistical inference. It's easy to find corner cases where one or the
other performs poorly, but for the most part, they are both reasonable
ways of drawing inferences.

The keys to other exercises either won't be made publically available,
or they won't include so much explanation.



\end{document}
